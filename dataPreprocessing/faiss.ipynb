{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryamsyeda/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# the following is to import the environment \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# to load all the environment variables\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "openai_api_keys = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('knowledgeBase1.txt')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "chunk_size=500,\n",
    "chunk_overlap=20\n",
    ")\n",
    "total_split_docs= text_splitter.split_documents(docs)\n",
    "len(total_split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x103185700>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x1222acb50>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "db =   FAISS.from_documents(total_split_docs,embedding)\n",
    "db.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n"
     ]
    }
   ],
   "source": [
    "query1 = 'what does LLM stand for?'\n",
    "query1_answer = db.similarity_search(query1)\n",
    "print(query1_answer[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6a00f4e3-add3-4439-8991-8fce3d09e373', metadata={'source': 'knowledgeBase1.txt'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.'),\n",
       " Document(id='504d5801-3319-4374-9739-98326d35781c', metadata={'source': 'knowledgeBase1.txt'}, page_content='An LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64]'),\n",
       " Document(id='c2918401-4fb7-456e-94dc-c7d804d93d3a', metadata={'source': 'knowledgeBase1.txt'}, page_content='Generally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62]'),\n",
       " Document(id='bd1b3fd6-5ca1-429e-9ba3-28e001eb28b4', metadata={'source': 'knowledgeBase1.txt'}, page_content='{\\\\displaystyle i}\\n\\n. If the LLM is masked, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text surrounding token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the vector db to retriever class, so that we can use it with different langchain libraries\n",
    "\n",
    "retriever= db.as_retriever()\n",
    "docs_2 = retriever.invoke(query1)\n",
    "docs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='6a00f4e3-add3-4439-8991-8fce3d09e373', metadata={'source': 'knowledgeBase1.txt'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.'),\n",
       "  0.8950789),\n",
       " (Document(id='504d5801-3319-4374-9739-98326d35781c', metadata={'source': 'knowledgeBase1.txt'}, page_content='An LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64]'),\n",
       "  0.91972095),\n",
       " (Document(id='c2918401-4fb7-456e-94dc-c7d804d93d3a', metadata={'source': 'knowledgeBase1.txt'}, page_content='Generally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62]'),\n",
       "  1.1127019),\n",
       " (Document(id='bd1b3fd6-5ca1-429e-9ba3-28e001eb28b4', metadata={'source': 'knowledgeBase1.txt'}, page_content='{\\\\displaystyle i}\\n\\n. If the LLM is masked, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text surrounding token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n.'),\n",
       "  1.1161581)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in FAISS, we can do similarity search with score to get the L2 score or manhattan score , the less the better\n",
    "docs_and_score = db.similarity_search_with_score(query1)\n",
    "docs_and_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RAG PIPELINE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# #  Initializing prompt\n",
    "\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "    \tUse the following pieces of retrieved context to answer the question.\n",
    "    \tIf you don't know the answer,say NAI MALUM .\n",
    "\n",
    "    \tQuestion: {question}\n",
    "\n",
    "    \tContext: {context}\n",
    "\n",
    "    \tAnswer:\n",
    "    \t\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate({prompt})\n",
    "\n",
    "# Initializing an LLM\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    api_key = openai_api_keys\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"This code defines a chain where input documents are first formatted,\n",
    "then passed through a prompt template,\n",
    "and finally processed by an LLM.\"\"\"\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "\tRunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "\t| prompt_template\n",
    "\t| llm\n",
    "\t)\n",
    "\"\"\"This code creates a parallel process:\n",
    "one retrieves the context (using a retriever),\n",
    "and the other passes the question through unchanged.\n",
    "The results are then combined and assigned to the variable `answer` using the `rag_chain_from_docs` processing chain.\"\"\"\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "\t{\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters and are trained with self-supervised learning on a vast amount of text.\n",
      "{'context': [Document(id='504d5801-3319-4374-9739-98326d35781c', metadata={'source': 'knowledgeBase1.txt'}, page_content='An LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64]'), Document(id='6a00f4e3-add3-4439-8991-8fce3d09e373', metadata={'source': 'knowledgeBase1.txt'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.'), Document(id='c2918401-4fb7-456e-94dc-c7d804d93d3a', metadata={'source': 'knowledgeBase1.txt'}, page_content='Generally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62]'), Document(id='bd1b3fd6-5ca1-429e-9ba3-28e001eb28b4', metadata={'source': 'knowledgeBase1.txt'}, page_content='{\\\\displaystyle i}\\n\\n. If the LLM is masked, then \"context for token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n\" is the segment of text surrounding token \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n.')], 'question': 'what is llm', 'answer': AIMessage(content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters and are trained with self-supervised learning on a vast amount of text.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 288, 'total_tokens': 339, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-f943c56d-12c2-446c-9936-c97be2152ee3-0', usage_metadata={'input_tokens': 288, 'output_tokens': 51, 'total_tokens': 339, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain_with_source.invoke(\"what is llm\")\n",
    "print(response['answer'].content)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
